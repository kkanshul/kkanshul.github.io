<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="StyleSheet" href="track_transfer_files/style.css" type="text/css" media="all"> 

<title>Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</title> 
<style type="text/css">
#primarycontent h1 {
	font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
	text-align: center;
}
#primarycontent p {
	text-align: center;
}
#primarycontent {
	text-align: justify;
}
#primarycontent p {
	text-align: justify;
}
#primarycontent p iframe {
	text-align: center;
}
.featart {
  margin:4px;
}
</style>
<script src="track_transfer_files/analytics_002.js" async=""></script><script async="" src="track_transfer_files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-32372780-2', 'auto');
  ga('send', 'pageview');

</script>
<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>
</head> 
<body itemscope="" itemtype="http://schema.org/ScholarlyArticle"> 
<div id="primarycontent"> 
<p class="hiddenDiv" itemprop="url"></p>
<h1 itemprop="name" align="center"><strong>Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</strong></h1>
<h5 align="center"><img src="track_transfer_files/track_transfer.jpg" itemprop="image" alt="teaserImage" width="600"></h5> <p align="justify">Main idea: (top) Automatically tracked objects (yellow
and blue boxes) in weakly-labeled videos without any human initialization.
(bottom) Discriminative visual regions (green boxes)
mined in weakly-labeled training images. For each discriminative
region, we find its best matching region across all videos, and retrieve
its overlapping tracked object box (yellow dotted box) back
to the image. The retrieved boxes are used as pseudo ground-truth
to train an object detector. Our approach improves object localization
by expanding the initial visual region beyond a small object
part (bottom-left) or removing the surrounding context (bottomright).
In practice, we combine the retrieved boxes from multiple
visual regions in an image to produce its best box.</p>
<p style="text-align:center;margin-bottom:-15px;font-size:1em;font-weight:bold;">In <a href="http://cvpr2016.thecvf.com/" target="_blank">CVPR 2016</a></p>
<h3>People</h3>

<ul id="people" itemprop="accountablePerson">
<li><a href="http://krsingh.cs.ucdavis.edu">Krishna Kumar Singh</a></li>
<li><a href="http://fanyix.cs.ucdavis.edu/">Fanyi Xiao</a></li>
<li><a href="http://web.cs.ucdavis.edu/~yjlee/">Yong Jae Lee</a></li>
</ul>
<h3>Abstract</h3>


<p style="padding-left: 10px;	padding-right: 10px;">
The status quo approach to training object detectors requires
expensive bounding box annotations. Our framework
takes a markedly different direction: we transfer tracked object
boxes from weakly-labeled videos to weakly-labeled images
to automatically generate pseudo ground-truth boxes,
which replace manually annotated bounding boxes. We
first mine discriminative regions in the weakly-labeled image
collection that frequently/rarely appear in the positive/negative
images. We then match those regions to videos
and retrieve the corresponding tracked object boxes. Finally,
we design a hough transform algorithm to vote for
the best box to serve as the pseudo GT for each image, and
use them to train an object detector. Together, these lead to
state-of-the-art weakly-supervised detection results on the
PASCAL 2007 and 2010 datasets.
</p>


<h3>Paper</h3>
<table><tbody><tr>
  <td>
  </td>
  <td valign="middle">
    <p style="text-align:left;margin-top:5px;">
    <span style="font-size:12px">
    </span></p>
    <p>
    </p>

<p style="margin-top:10px;">

</p><p style="text-align:left;"><span style="font-size:4px;">&nbsp;<br></span> Krishna Kumar Singh,  Fanyi Xiao, Yong Jae Lee <br><a href="https://arxiv.org/abs/1604.05766"><b>Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</b></a> <br>In <i>CVPR 2016</i>
   <a href="javascript:togglevis('krishna16')" id="showbib">[Show BibTex]</a><br>
  </p></td>
</tr></tbody></table>
      <table class="bibtex" style="display:none;margin-top:20px;" id="krishna16"><tbody><tr><td>
          <pre>@inproceedings{krishna-cvpr2016,
  title = {Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection},
  author = {Krishna Kumar Singh and Fanyi Xiao and Yong Jae Lee},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2016}
}
          </pre>
       <!--</div>-->
       </td></tr></tbody></table>
<p></p>

<!--<h3 style="clear:both">Video</h3><br/><br/>
  <iframe width="800" height="450" src="http://www.youtube.com/embed/s5-30NKSwo8?version=3&fs=1&feature=player_embedded&fs=1&hd=1&ap=%2526fmt%3D22" frameborder="1" allowfullscreen></iframe>
</p>-->


<h3>Additional Materials</h3>
<p style="padding-left: 10px;	padding-right: 10px;">

</p><ul>
<li><a href=""><b>Poster</b></a> Coming Soon!</li>
<li><a href=""><b>Code</b></a> Coming Soon!</li>
</ul>


<h3>Interesting Results</h3>
<p style="padding-left: 10px;	padding-right: 10px;">
<img src="track_transfer_files/qualitative_results.jpg" itemprop="image" width="800" />
<p align="justify"> Qualitative results on the VOC 2007 train+val set. In each image pair, the first image shows a heatmap of the transferred video
object boxes and the second image shows the final selected pseudo ground-truth box. Our approach accurately discovers the spatial extent
of the object-of-interest in most of the images. The last column shows mis-localized examples. Our approach can fail when there are
multiple instances of the same object category in the image (e.g., aeroplane, dog, horse, train) or when the objectâ€™s appearance is very
different from that found in videos (e.g., car). </p>
</p>


<h3 style="clear:both">Acknowledgments</h3>
<p style="padding-left: 10px; padding-right: 10px;">
This work was supported in part by
an Amazon Web Services Education Research Grant and
GPUs donated by NVIDIA.
</p>
<p></p>
<p style="padding-left: 10px; padding-right: 10px;">
Comments, questions to <a href="mailto:krsingh@ucdavis.edu" target="_blank">Krishna Kumar Singh</a></p>
</div>





</body></html>
